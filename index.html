<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
  <style>
    .navA{
      display: inline-block;
      margin-right: 13px;
      font-size: 16px;
      font-weight: 700;
      color: #000;
      text-decoration: none;
      padding: 5px ;
      border: #000 1px solid;
    }
    .navA:hover{
      color: #fff;
      background-color: #000;
    }
  </style>

  <title>Dongkuan (DK) Xu</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dongkuan (DK) Xu / 胥栋宽</name>
              </p>
              <p>Hello! I am an Assistant Professor at <a href="https://www.csc.ncsu.edu/"> NC State CS</a>, leading the <a href="https://www.csc.ncsu.edu/"> NCSU Reliable & Efficient Computing Lab</a> <em>(web under construction)</em> and working on deep learning, machine learning, and natural language processing. I received my Ph.D. at <a href="http://www.psu.edu/">Penn State</a>, where I was advised by <a href="https://faculty.ist.psu.edu/xzz89/">Xiang Zhang</a>. I received my M.S. in Optimization and B.E. at the <a href="https://www.ucas.ac.cn">University of Chinese Academy of Sciences</a> and <a href="http://www.ruc.edu.cn/">Renmin University of China</a>, respectively.
              </p>
              <p>
                I has been collaborating with <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/">Microsoft Research</a> exploring neural architecture search (NAS) and hyperparameter optimization (HPO) for <a href="https://en.wikipedia.org/wiki/Foundation_models">Foundation Models</a>, and with <a href="https://research.google/">Google Research</a> to enable scalable and adaptive learning for <a href="https://huggingface.co/blog/vision_language_pretraining">Vision-Language Models</a>. I was a research scientist at <a href="https://moffett.ai">Moffett AI</a>, investigating low-resource model compression. I also spent some wonderful time at <a href="http://www.nec-labs.com">NEC Labs America</a> on contrastive learning and multi-task learning.
              </p>
              <p>
                Other than my work, I am a big fan of American football. I love <a href="https://gopsusports.com/sports/football">Nittany Lions</a>, <a href="https://www.giants.com">New York Giants</a>, and <a href="https://www.dallascowboys.com/">Dallas Cowboys</a>. I also like workout and soccer ball.
              </p>
              <p style="text-align:center">
                <a href="mailto:dxu27@ncsu.edu">Email</a> &nbsp/&nbsp
                <a href="CV_DK.pdf">CV (April 2023)</a> &nbsp/&nbsp
                <a href="https://twitter.com/DongkuanXu">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2mJ7jpcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/dongkuan-dk-xu-05038087/">LinkedIn</a> &nbsp&nbsp
              </p>
              <p>
                <b><font color="red">I'm actively looking for multiple PhDs / interns to work in Reliable & Efficient AI <a href="https://www.1point3acres.com/bbs/thread-936744-1-1.html">(一亩三分地 I,</a> <a href="https://www.1point3acres.com/bbs/thread-885798-1-1.html">一亩三分地 II)</a>. Feel free to send me your CV. Once we have a commitment to each other, trust me I will do my best to help you!</font></b> &nbsp&nbsp
              </p>
              <p>
                <font color="blue"><em>(I've received an amazingly large number of applications. Super thanks for everyone's interest! Interviews are in progress. Good luck~)</em></font> &nbsp&nbsp
              </p>


            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/DK.png"><img style="width:80%;max-width:80%" alt="profile photo" src="images/DK.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <div class="navbar" style="padding-left: 18px;">
          <a href="#Research" class="navA">Research</a>
          <a href="#News" class="navA">News</a>
          <a href="#Publications" class="navA">Publications</a>
          <a href="#Students" class="navA">Students</a>
          <a href="#Teaching" class="navA">Teaching</a>
          <a href="#Services" class="navA">Services</a>
          <a href="#Patent" class="navA">Patents</a>
          <a href="#Talks" class="navA">Talks</a>
          <a href="#Honors" class="navA">Awards</a>
          <!-- <a href="#Extracurricular" class="navA">Extracurricular Activities</a> -->
        </div>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>

          <td width="100%" valign="middle">

            <heading id="Research"><b>Research</b></heading>
            <p>
            My research is fundamentally grounded in exploring and advancing <b><font color="red">Landable Generative Artificial Intelligence (LAGAI)</font></b>, with particular emphasis on studying the decision reliability <b>(alignment, uncertainty, adaptability, robustness)</b>, resource efficiency <b>(data utilization, computation, parameter optimization)</b>, and autonomy of intelligent agents <b>(tools, planning, tasks)</b> in <b><font color="blue">Generative AI (ChatGPT, GPT-X, Diffusion Models) Systems</font></b>. My research group provides full-stack LAGAI solutions, ranging from theoretical optimization methods and data-centric strategies to the development of efficient deep learning techniques and the co-design of algorithms and hardware. My long-term research goal is to free AI from the manual-design-device-energy beast and democratize its application to serve a broader range of populations and real-world applications, equally, sustainably, and responsibly.
            </p>


            <ul>
            <li><p>
              <strong>Reliable Reasoning & Decision Making in Large Language Models</strong>
            </p>
            <li><p>
              <strong>Tool / Planning / Task Automation in Large Language Models</strong>
            </p>
            <li><p>
              <strong>Data Optimization/Augmentation/Selection to Improve Data Quality & Utility</strong>
            </p>
            <li><p>
              <strong>Algorithm-Hardware Co-design for AI Landing</strong>
            </p>
            <li><p> 
              <strong>Applications: NLP, Computer Vision, Agriculture, Education, Health</strong>
            </p>

            </ul>
          </td>
        </tr>

        <tr>

          <td width="100%" valign="middle">

            <heading><b><font color="red"><i>Call for Papers</i></font></b></heading>
            <p>
            <font color="red"><b>Workshop on Resource-Efficient Learning for Knowledge Discovery <a href="https://ncsu-dk-lab.github.io/workshops/relkd@2023/">(link)</a> to be held at KDD’23 (Workshop Co-Chair)</b></font>
            </p>
            <ul>
            <li><p>
              Welcomed topics include: data-efficient learning, algorithm/model-efficient learning, system/hardward-efficient learning, etc
            </p>
            <li><p>
              Submission Deadline: <b>June 07, 2023</b> <a href="https://cmt3.research.microsoft.com/RelKD2023/Submission/Index">(link)</a>
            </p>
            </ul>

            <p>
            <font color="red"><b>The First Workshop on DeepLearning-Hardware Co-Design for AI Acceleration <a href="https://ncsu-dk-lab.github.io/workshops/dcaa@2023/">(link)</a> to be held at AAAI’23 (Workshop Chair)</b></font>
            </p>
            <ul>
            <li><p>
              Welcomed topics include: model compression, deep learning acceleration, hard-soft co-design, applications, etc
            </p>
<!--             <li><p>
              Workshop is non-archival and permits under-review or concurrent submissions. Best Paper Awards will be selected
            </p> -->
            <li><p>
              Submission Deadline: <b>November 30, 2022</b> <a href="https://cmt3.research.microsoft.com/DCAA2023/Submission/Index">(link)</a>
            </p>
            </ul>

          </td>
        </tr>

        <tr>
          <td>
            <heading id="News"><b>News</b></heading>
              <div class="list scroll">
              <ul>
                <li>05/2023: Feel free to check out our latest work on Augmented LLM, <b><font color="red">ReWOO <a href="work/ReWOO_arxiv.pdf" style="color:blue;"></b>(paper)</a></font>!
                <li>05/2023: One paper was accepted to <a href="https://kdd.org/kdd2023/">KDD'23</a>!
                <li>05/2023: One paper was accepted to <a href="https://2023.aclweb.org/">ACL'23</a>!
                <li>04/2023: Our work, <b><font color="red">E-App</font></b>, was accepted to <a href="http://www.icccn.org/">ICCCN'23</a>! See u in Honolulu!
                <li>04/2023: One paper was accepted to <a href="http://www.icaibd.org/">ICAIBD'23</a>! Congrats to our undergrad, Zihan!
                <li>03/2023: Our work, <b><font color="red">Acc.DD <a href="https://arxiv.org/pdf/2212.06152.pdf" style="color:black;">(paper)</a></font></b>, was selected as a <b><font color="red">Highlight (2.5%)</font></b> of <a href="https://cvpr2023.thecvf.com/">CVPR'23</a>!
                <li>03/2023: Will co-chair <b> RelKD'23: Resource-Efficient Learning for Knowledge Discovery Workshop</b> @<a href="https://kdd.org/kdd2023/">KDD'23</a>.</li>
                <li>02/2023: Two papers on <b><font color="red">accelerating data/model learning</font></b> were accepted to <a href="https://cvpr2023.thecvf.com/">CVPR'23</a>. Stay tuned ;-)</li>
                <li>02/2023: Two papers on <b><font color="red">dynamic training</font></b> were accepted to <a href="https://www.dac.com/">DAC'23</a>. See u in San Francisco!</li>
                <li>01/2023: Our work, <b><font color="red">Calibrated Rigged Lottery<a href="https://arxiv.org/pdf/2302.09369.pdf" style="color:black;"></a></font></b>, was accepted to <a href="https://iclr.cc/">ICLR'23</a>.</li>
                <li>01/2023: Our work, <b><font color="red">Efficient Informed Proposals</font></b> for Discrete Distributions, was accepted to <a href="https://aistats.org/aistats2023/">AISTATS'23</a>.</li>
                <li>01/2023: Invited to give a talk at <a href="https://sites.google.com/site/boyuaneecs/efficient-ai-seminar-talk?authuser=0"><b><font color="black">Rutgers EFficient AI (REFAI) Seminar</font></b></a> on Feb 16, 2023.</li>
                <li>01/2023: Invited to give a talk at <a href="https://sites.google.com/site/boyuaneecs/efficient-ai-seminar-talk?authuser=0"><b><font color="black">2022 Annual Summit for High Tech High Growth Companies</font></b></a> on March 11, 2023.</li>
                <li>12/2022: Invited to serve as a journal reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a> and <a href="https://cacm.acm.org/">Communications of the ACM</a>.</li>
                <li>11/2022: Invited to serve as the PC Chair for <a href="https://www.mlnlp2022.com/"><b><font color="red">MLNLP 2022.</font></b></a> Super welcome to attend (online & free)!</li>
                <li>11/2022: Two papers were accepted to <a href="https://aaai.org/Conferences/AAAI-23/">AAAI'23</a>. See you in DC in February!</li>
                <li>10/2022: Invited to serve as a TPC member for <a href="https://www.isqed.org/">ISQED'23</a>.</li>
                <li>09/2022: Will chair <b>The First Workshop on DeepLearning-Hardware Co-Design for AI Acceleration</b> with <a href="https://aaai.org/Conferences/AAAI-23/">AAAI'23</a>!
                <li>09/2022: Our work, <b><font color="red">AutoDistil <a href="https://arxiv.org/pdf/2201.12507.pdf" style="color:black;">(paper)</a></font></b>, was accepted to <a href="https://nips.cc/">NeurIPS'22</a>. See u in New Orleans!
                <li>09/2022: Invited to give a talk at <a href="https://www.cis.um.edu.mo/"><b><font color="black">the CIS Department of the University of Macau</font></b></a>.</li>
                <li>07/2022: Will chair a Research session (<b><font color="black">Deep Learning: New Architectures and Models</font></b>) and an Applied Data Science session (<b><font color="black">Scalable, Distributed Systems & Trustable AI</font></b>) of <a href="https://kdd.org/kdd2022/index.html">KDD'22. </a>Super welcome!</li>
                <li>07/2022: Will be teachinng <b><font color="red">CSC 791 Advanced Topics in Efficient Deep Learning</font></b> at NC State this fall. Feel free to attend!</li>
                <li>07/2022: One paper, <b><font color="red">S4: a High-sparsity, High-performance AI Accelerator <a href="https://arxiv.org/pdf/2207.08006.pdf" style="color:black;">(paper)</a></font></b>, was accepted to <a href="https://www.sparseneural.net/home">SNN'22</a>!</li>
                <li>07/2022: Invited to serve as a (Senior) PC member for <a href="https://aaai.org/Conferences/AAAI-23/">AAAI'23.</a> and <a href="https://iclr.cc/Conferences/2023/CallForPapers">ICLR'23.</a></li>
                <li>06/2022: Invited to serve as a Column Editor for <a href="https://sigai.acm.org/main/ai-matters/"><b><font color="red">ACM SIGAI Newsletter.</font></b></a></li>
                <li>06/2022: Invited to give a talk at <a href="https://www.pinterestcareers.com/departments/"><b><font color="black">Pinterest (Pinterest Machine Learning Lunch)</font></b></a> on August 18, 2022.</li>
                <li>06/2022: Invited to give a talk at <a href="https://www.siat.ac.cn/"><b><font color="black">中科院深圳先进技术研究院</font></b></a> on June 27, 2022.</li>
                <li>06/2022: Invited to serve as a PC member for <a href="https://www.cikm2022.org/">WSDM'23</a>, <a href="https://logconference.org/">LoG'22</a>, and <a href="https://www.aacl2022.org/">AACL-IJCNLP'22</a>.</li>
                <li>05/2022: Invited to serve as a PC member for <a href="https://coling2022.org/">COLING'22</a> and a reviewer for the journal <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">TNNLS</a>.</li>
                <li>05/2022: Invited to give a talk at <a href="https://en.wikipedia.org/wiki/A9.com"><b><font color="black">Amazon Search (A9)</font></b></a> on May 20, 2022. ("5.20" >.< "我爱你")</li>
                <li>04/2022: Invited to give a talk at <a href="http://www.thejiangmen.com/"><b><font color="black">将门创投</font></b></a> on May 24, 2022. Welcome!</li>
                <li>04/2022: Invited to give a talk at <a href="https://www.vanderbilt.edu/"><b><font color="black">Vanderbilt University</font></b></a>'s Machine Learning Lunch Seminar on May 09, 2022.</li>
                <li>04/2022: Invited to give a talk at <a href="https://www.ruc.edu.cn/"><b><font color="black">Renmin University of China</font></b></a> in May 2022.</li>
                <li>04/2022: Invited to give a talk at <a href="https://www.szu.edu.cn/"><b><font color="black">Shenzhen University</font></b></a> in May 2022.</li>
                <li>04/2022: A new US patent application: <a href="https://www.freepatentsonline.com/y2022/0101118.html"><b><font color="red">Bank-balanced-sparse Activation for Deep NN Models.</font></b></a></li>
                <li>04/2022: Invited to give a talk at <a href="https://uconn.edu/"><b><font color="black">University of Connecticut</font></b></a> on April 27, 2022.</li>
                <li>04/2022: Invited to give a talk at <a href="https://www.ucas.ac.cn/"><b><font color="black">UCAS (中国科学院大学)</font></b></a> on April 25, 2022.</li>
                <li>04/2022: Invited to give a talk at <a href="https://www.nyit.edu/"><b><font color="black">New York Institute of Technology</font></b></a>'s Research Seminar Series.</li>
                <li>04/2022: Organizing MLNLP Community's <a href="https://mp.weixin.qq.com/s/f8k4n8lmvv2q-2VIJuUNHA"><b><font color="black">6th Academic Seminar.</font></b></a></li>
                <li>04/2022: <b><font color="black">Third place winner (Eng.)</font></b> in the 37rd annual PSU Graduate Exhibition (<a href="https://gradschool.psu.edu/exhibition/awards/#research">News</a>).</li>
                <li>03/2022: Invited to serve as a PC member for <a href="https://nips.cc/Conferences/2022/CallForPapers">NeurIPS'22</a>.</li>
                <li>02/2022: One paper, <b><font color="black">Sparse Progressive Distillation <a href="https://github.com/shaoyiHusky/SparseProgressiveDistillation" style="color:red;">(code,</a> <a href="https://arxiv.org/pdf/2110.08190.pdf" style="color:red;">paper)</a></font></b>, was accepted to <a href="https://www.2022.aclweb.org/">ACL'22</a>!</li>
                <li>02/2022: Invited to serve as a PC member for <a href="https://www.cikm2022.org/">CIKM'22</a>.</li>
                <li>12/2021: Thanks to <a href="https://mp.weixin.qq.com/s/i071V4nDks_aagUWf497xQ">MLNLP</a> (机器学习与自然语言处理) for reporting our work SparseBERT.</li>
                <li>12/2021: Code released for <b><font color="red">SparseBERT (NAACL'21) <a href="https://github.com/DerronXu/SparseBERT" style="color:red;">(code</a>, <a href="https://arxiv.org/abs/2104.08682" style="color:red;">paper)</a>!</font></b> Feel free to use it!</li>
                <li>12/2021: Invited to serve as a PC member for <a href="https://icml.cc/Conferences/2022">ICML'22</a>.</li>
                <li>12/2021: Invited to give a talk <b><font color="black">"Parameter Efficiency: Democratizing AI at Scale"</font></b> at <a href="https://www.brandeis.edu/computer-science/"><b><font color="black">Brandeis University</font></b></a> <a href="https://github.com/DerronXu/Talks/blob/master/Talk_Parameter_Efficiency_Democratize_AI_at_Scale_Brandeis.pdf">(slides)</a>.</li>
                <li>11/2021: Invited to serve as a PC member for <a href="https://kdd.org/kdd2022/">KDD'22</a> (both Research and Applied Science Tracks).</li>
                <li>10/2021: Our <a href="https://mp.weixin.qq.com/s/hs85C_Cffb1d1KwoacnXJQ">ML&NLP</a> academic community is officially launched <b><font color="black">(>500k followers)</font></b>.</li>
                <li>10/2021: Received IST Fall 2021 Travel Award.</li>
                <li>09/2021: Our work, <b><font color="black">InfoGCL</font></b>, was accepted to <a href="https://neurips.cc/">NeurIPS'21</a>!
                <li>08/2021: Invited to serve as PC member for <a href="https://aaai.org/Conferences/AAAI-22/">AAAI'22</a>, <a href="https://www.aclweb.org/portal/">ACL Rolling Review'22.</a>, <a href="https://www.siam.org/conferences/cm/conference/sdm22">SDM'22</a>.</li>
                <li>07/2021: Received complimentary ACM student membership. Thanks you ACM!.</li>
                <li>06/2021: Invited to serve as a PC member for <a href="https://iclr.cc/Conferences/2022/CallForPapers">ICLR'22</a>, <a href="http://www.wsdm-conference.org/2022/">WSDM'22</a>, <a href="https://www.ijcai.org/future_conferences">IJCAI-ECAI'22</a>.</li>
                <li>05/2021: Received NAACL 2021 Scholarship.</li>
                <li>05/2021: One paper was accepted to <a href="https://2021.aclweb.org/">ACL'21</a>!</li>
                <li>05/2021: Excited to join <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/"><b><font color="black">Microsoft Research</font></b></a> as a research intern working on neural architecture search!</li>
              	<li>04/2021: Gave a talk titled "BERT Pruning: Structural vs. Sparse" at <a href="https://www.brandeis.edu/computer-science/"><b><font color="black">Brandeis University</font></b></a> <a href="https://github.com/DerronXu/Talks/blob/master/Talk_BERT_Pruning_Structural_vs_Sparse_Brandeis.pdf">(slides)</a>.</li>
              	<li>04/2021: Gave a talk titled "BERT, Compression and Applications" at <a href="https://en.xiaopeng.com"><b><font color="black">Xpeng Motors</font></b></a> (小鹏汽车) <a href="https://github.com/DerronXu/Talks/blob/master/Talk_BERT_Compression_and_Application_Xpeng.pdf">(slides)</a>.</li>
                <li>03/2021: My application to <a href="https://www.siam.org/conferences/cm/program/special-events/sdm21-special-events">SDM'21 Doctoral Forum</a> has been accepted. See you in May!</li>
                <li>03/2021: Received a SIAM Student Travel Award to attend <a href="https://www.siam.org/conferences/cm/conference/sdm21">SDM'21</a>.</li>
                <li>03/2021: Our work, <b><font color="black">SparseBERT</font></b>, was accepted to <a href="https://2021.naacl.org/">NAACL'21</a>! Along with <b><font color="black">three U.S. patent applications</font></b>!</li>
                <li>03/2021: Invited to serve as a PC member for <a href="https://nips.cc/Conferences/2021">NeurIPS'21</a>, <a href="https://2021.emnlp.org/">EMNLP'21</a>, <a href="https://www.cikm2021.org/">CIKM'21</a>.</li>
                <li>03/2021: Received IST Spring 2021 Travel Award.</li>
                <li>12/2020: One paper was accepted to <a href="https://www.siam.org/conferences/cm/conference/sdm21">SDM'21</a>. See you virtually in April!</li>
                <li>12/2020: Invited to serve as a Senior PC member for <a href="https://ijcai-21.org">IJCAI'21</a>.</li>
                <li>12/2020: Four papers were accepted to <a href="https://aaai.org/Conferences/AAAI-21/">AAAI'21</a>. See you virtually in February!</li>
                <li>12/2020: Invited to serve as a PC member for <a href="https://icml.cc/Conferences/2021">ICML'21</a>, <a href="https://www.kdd.org/kdd2021/">KDD'21</a>, <a href="https://2021.naacl.org/">NAACL'21</a>, <a href="https://www.ijcnn.org">IJCNN'21</a>.</li>
                <li>09/2020: Our work, <a href="https://github.com/flyingdoog/PGExplainer"><b><font color="black">PGExplainer</font></b></a>, was accepted to <a href="https://nips.cc">NeurIPS'20</a>.</li>
                <li>08/2020: Invited to serve as a PC member for <a href="https://aaai.org/Conferences/AAAI-21/">AAAI'21</a>, <a href="https://2021.eacl.org">EACL'21</a>, a journal reviewer for <a href="https://www.sciencedirect.com/journal/information-fusion">Information Fusion</a>.</li>
                <li>08/2020: Received KDD 2020 Student Registration Award.</li>
                <li>06/2020: Invited to serve as a reviewer for <a href="https://nips.cc">NeurIPS'20</a>.</li>
                <li>05/2020: Happy to join <a href="https://moffett.ai/"><b><font color="black">Moffett AI</font></b></a> as an intern research scientist.</li>
                <li>04/2020: One paper was accepted to <a href="https://sigir.org/sigir2020/">SIGIR'20</a>.</li>
                <li>03/2020: Invited to serve as a PC member for <a href="https://2020.emnlp.org">EMNLP'20</a>, <a href="https://www.kdd.org/kdd2020/">KDD'20</a>, <a href="https://cikm2020.org/">CIKM'20</a>, <a href="http://aacl2020.org">AACL-IJCNLP'20</a>.</li>
                <li>02/2020: Received IST Spring 2020 Travel Award.</li>
                <li>12/2019: Invited to serve as a PC member for <a href="https://ijcai20.org">IJCAI'20</a>, <a href="https://wcci2020.org">IJCNN'20</a>.</li>
                <li>12/2019: Received AAAI 2020 Student Scholarship.</li>
                <li>11/2019: Two papers were accepted to <a href="https://aaai.org/Conferences/AAAI-20/">AAAI'20</a>. See you in the Big Apple!</li>
                <li>08/2019: Invited to serve as a PC member for <a href="https://aaai.org/Conferences/AAAI-20/">AAAI'20</a>.</li>
                <li>08/2019: One paper was accepted to <a href="http://icdm2019.bigke.org">ICDM'19</a>.</li>
                <li>05/2019: One paper was accepted to <a href="https://ijcai19.org">IJCAI'19</a>.</li>
                <li>05/2019: Happy to join <a href="http://www.nec-labs.com/"><b><font color="black">NEC Labs America</font></b></a> as a research intern.</li>
                <li>03/2019: Received IST Spring 2019 Travel Award.</li>
                <li>01/2019: Grateful to receive <b><font color="black">The Award for Excellence in Teaching, IST</font></b> (<a href="https://news.psu.edu/story/559778/2019/02/19/academics/college-ist-recognizes-outstanding-graduate-students-annual-awards">News</a>).</li>
                <li>01/2019: Invited to serve as a PC member for <a href="https://www.ijcnn.org">IJCNN'19</a>.</li>
                <li>12/2018: One paper was accepted to <a href="https://www.siam.org/conferences/cm/conference/sdm19">SDM'19</a>. See you in Calgary!</li>
                <li>05/2018: Started working at <a href="http://www.nec-labs.com/"><b><font color="black">NEC Labs America</font></b></a> as a research intern.</li>
                <li>11/2017: Invited to serve as a PC member for <a href="http://www.ecomp.poli.br/~wcci2018/">IJCNN'18</a>.</li>
              </ul>
              </div>
          </td>
        </tr>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading id="Publications"><b>Publications</b></heading>
              <!-- <h3>Graduate</h3> -->
<!--               <p>
                I'm interested in computer vision, machine learning, optimization, and image processing.
                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.
                Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2023</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/ReWOO.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models</papertitle>
              </a>
              <br>
              <U>B. Xu</U>, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, <b>D. Xu</b>
<!--               <br>
              <em><b>[xxx 2023]</b></em> <i>xxx xxx</i> -->
              <br>
              <a href="work/ReWOO_arxiv.pdf">PDF</a>
              <p>We present a modular ALM framework to solve multi-step reasoning by decoupling reasoning from tool feedback and observations. Theoretical decomposition of prompt tokens establishes that our method <font color="red">substantially reduces prompting redundancy in prevailing Thought-Action-Observation ALM systems</font>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/edugpt.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Exploring the Augmented Large Language Model with Mathematical tools in Personalized and Efficient Education</papertitle>
              </a>
              <br>
              <U><a href="https://zihandong.wordpress.ncsu.edu/">Zihan Dong</a></U> (<b><font color="red">Undergrad at NC State</font></b>), <b>D. Xu</b>
              <br>
              <em><b>[ICAIBD 2023]</b></em> <i>The 6th International Conference on Artificial Intelligence and Big Data</i>
              <p>We propose to <font color="red">augment ChatGPT with math performance assessments</font>, which facilitate the creation of customized learning experiences based on the needs of each student. This study explores how ChatGPT personalizes the learning experience, how it can be augmented with math and physical performance, and how educators can ensure that the LLM algorithm is unbiased.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/ACC_DD.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">Accelerating Dataset Distillation via Model Augmentation</font></papertitle>
              </a>
              <br>
              <U>L. Zhang*</U>, J. Zhang*, B. Lei, S. Mukherjee, X.Pan, B.Zhao, C. Ding, Y. Li, <b>D. Xu</b>
              <br>
              <em><b>[CVPR 2023]</b></em> <i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>
              <br>
              <b><font color="red">Highlight Paper (2.5%)</font></b>
              <br>
              <a href="https://arxiv.org/pdf/2212.06152.pdf">PDF</a>
              <p>We propose two model augmentation techniques, i.e. using early-stage models and weight perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20× speedup.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/MuE.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>You Need Multiple Exiting: <font color="red">Dynamic Early Exiting for Accelerating Unified Vision Language Model</font></papertitle>
              </a>
              <br>
              <U>S. Tang</U>, Y. Wang, Z. Kong, T. Zhang, Y. Li, C. Ding, Y. Wang, Y. Liang, <b>D. Xu</b>
              <br>
              <em><b>[CVPR 2023]</b></em> <i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>
              <br>
              <a href="https://arxiv.org/pdf/2211.11152.pdf">PDF</a>
              <p>We propose a novel early exiting strategy based on cascading input similarity with valid assumptions on saturation states in visual-language models, a pioneering exploration of extending early exiting selection to encoders and decoders of sequence-to-sequence architectures.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/CaliRare.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Towards Reliable Rare Category Analysis on Graphs via Individual Calibration</papertitle>
              </a>
              <br>
              L. Wu, B. Lei, <b>D. Xu</b>, D. Zhou
              <br>
              <em><b>[KDD 2023]</b></em> <i>The 29th SIGKDD Conference on Knowledge Discovery and Data Mining</i>
              <br>
              <!-- <a href="https://arxiv.org/pdf/2211.07886.pdf">PDF</a> -->
              <p>How can we quantify the uncertainty in the learning process and enable <font color="red">reliable rare category analysis?</font> We propose an end-to-end method that jointly learns the characterizations of rare categories and calibrates the confidence.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/ODQA.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>A Survey for Efficient Open Domain Question Answering</papertitle>
              </a>
              <br>
              Q. Zhang, S. Chen, <b>D. Xu</b>, Q. Cao, X, Chen, T. Cohn, M. Fang
              <br>
              <em><b>[ACL 2023]</b></em> <i>The 61th Annual Meeting of the Association for Computational Linguistics</i>
              <br>
              <a href="https://arxiv.org/pdf/2211.07886.pdf">PDF</a>
              <p>We provide a survey of recent advances in <font color="red">the efficiency of ODQA models</font>. We walk through the ODQA models and conclude the core techniques on efficiency. Quantitative analysis on memory cost, processing speed, accuracy and overall comparison are given.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/CigL.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">Calibrating the Rigged Lottery: Making All Tickets Reliable</font></papertitle>
              </a>
              <br>
              <U>B. Lei</U>, R. Zhang, <b>D. Xu</b>, B. K. Mallick
              <br>
              <em><b>[ICLR 2023]</b></em> <i>The 11th International Conference on Learning Representations</i>
              <br>
              <a href="https://arxiv.org/pdf/2302.09369.pdf">PDF</a>
              <p> We for the first time identify and study the reliability problem of sparse training and find that sparse training exacerbates the over-confidence problem of DNNs. We then develop a new sparse training method, CigL, to produce more reliable sparse models, which can simultaneously maintain or even improve accuracy with only a slight increase in computational and storage burden.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/eapp.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>E-App: Adaptive mmWave Access Point Planning with Environmental Awareness in Wireless LANs</papertitle>
              </a>
              <br>
              Y. Liu, M. Chen, <b>D. Xu</b>, Z. Yang, S. Zhao
              <br>
              <em><b>[ICCCN 2023]</b></em> <i>The 32nd International Conference on Computer Communications and Networks</i>
              <br>
              <!-- <a href="https://dongkuanx27.github.io/">PDF (to appear)</a> -->
              <p>To enable ultra-high throughputs while addressing the potential blockage problem, maintaining an adaptive access point (AP) planning is critical to mmWave networking. We develop an adaptive AP planning (E-app) approach that can accurately sense the environment dynamics, reconstruct the obstacle map, and then predict the placements of mmWave APs adaptively.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/DSTEE.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Dynamic Sparse Training via Balancing the <font color="red">Exploration-Exploitation Trade-off</font></papertitle>
              </a>
              <br>
              <U>S. Huang</U>, B. Lei, <b>D. Xu</b>, H. Peng, Y. Sun, M. Xie, C. Ding
              <br>
              <em><b>[DAC 2023]</b></em> <i>The 60th Design Automation Conference</i>
              <br>
              <a href="https://arxiv.org/pdf/2211.16667.pdf">PDF</a>
              <p>To assist explainable sparse training, we propose important weights exploitation and weights coverage exploration to characterize sparse training. Our method does not need to train dense models, achieving up to 95% sparsity ratio and even higher accuracy than dense training, with same amount of iterations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/NDSNN.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">Neurogenesis Dynamics-inspired</font> Spiking Neural Network Training Acceleration</papertitle>
              </a>
              <br>
              <U>S. Huang</U>, H. Fang, K. Mahmood, B. Lei, N. Xu, B. Lei, Y. Sun, <b>D. Xu</b>, W. Wen, C. Ding
              <br>
              <em><b>[DAC 2023]</b></em> <i>The 60th Design Automation Conference</i>
              <br>
              <p> We propose an energy efficient spiking neural network training workflow, and design a new drop-andgrow strategy with decreasing number of non-zero weights in the process of dynamically updating sparse mask. We demonstrate extremely high sparsity (i.e., 99%) model performance in SNN based vision tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/MANA.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Efficient Informed Proposals for Discrete Distributions via Newton’s Series Approximation</papertitle>
              </a>
              <br>
              <U>Y. Xiang*</U>, <U>D. Zhu*</U>, B. Lei, <b>D. Xu</b>, R. Zhang
              <br>
              <em><b>[AISTATS 2023]</b></em> <i>The 26th International Conference on Artificial Intelligence and Statistics</i>
              <br>
              <a href="http://www.personal.psu.edu/dux19/">PDF</a>
              <p> We develop a gradient-like proposal for any discrete distribution without this strong requirement. Built upon a locally-balanced proposal, our method efficiently approximates the discrete likelihood ratio via a Newton’s series expansion to enable a large and efficient exploration in discrete spaces.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/AGENT.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction</font></papertitle>
              </a>
              <br>
              B. Lei, <b>D. Xu</b>, R. Zhang, S. He, B. K. Mallick
              <br>
              <!-- <em><b><i>arXiv preprint arXiv:2301.03573 (2023)</i></b></em> -->
              <i>arXiv preprint arXiv:2301.03573 (2023)</i>
              <br>
              <a href="https://arxiv.org/pdf/2301.03573.pdf">PDF</a>
              <p>We develop an adaptive gradient correction method to accelerate and stabilize the convergence of sparse training. Theoretically, we prove that our method can accelerate the convergence rate of sparse training.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/DisVar.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Improving Long-tailed Classification by Disentangled Variance Transfer</papertitle>
              </a>
              <br>
              Y. Tian, W. Gao, Q. Zhang, P. Sun, <b>D. Xu</b>
              <br>
              <em><b><i>Internet of Things</i></b></em> 
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S2542660523000100">PDF</a>
              <p>We propose a class-based covariance transfer method from the perspective of disentangling to transfer covariance information in long-tailed classification task.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/Auto-CAM.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Auto-CAM: Label-Free Earth Observation Imagery Composition and Masking Using Spatio-Temporal Dynamics</papertitle>
              </a>
              <br>
              Y. Xie, Z. Li, H. Bao, X. Jia, <b>D. Xu</b>, X. Zhou, S. Skakun
              <br>
              <em><b>[AAAI 2023]</b></em> <i>The 37th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="http://www.personal.psu.edu/dux19/">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a>
              <p>We propose an autonomous image composition and masking method for <b>cloud masking</b>, a fundamental task in Earth observation problems across social sectors such as <b>agriculture, energy, and water</b>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/InfoTS.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Time Series Contrastive Learning with Information-Aware Augmentations</papertitle>
              </a>
              <br>
              D. Luo, W. Cheng, Y. Wang, <b>D. Xu</b>, J. Ni, W. Yu, X. Zhang, Y. Liu, Y. Chen, H. Chen, X. Zhang
              <br>
              <em><b>[AAAI 2023]</b></em> <i>The 37th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="http://www.personal.psu.edu/dux19/">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a>
              <p>We propose an adaptive data augmentation method to avoid ad-hoc choices or painstakingly trial-and-error tuning for time series representation learning.</p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2022</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/AutoDistil.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="http://www.personal.psu.edu/dux19/" id="3DSP"> -->
              <a id="3DSP">
                <papertitle><font color="red"> AutoDistil:</font> Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models </papertitle>
              </a>
              <br>
              <b>D. Xu</b>, S. Mukherjee, X. Liu, D. Dey, W. Wang, X. Zhang, A. H. Awadallah, J. Gao
              <br>
              <em><b>[NeurIPS 2022]</b></em> <i>The 36th Conference on Neural Information Processing Systems</i>
              <br>
              <a href="https://arxiv.org/pdf/2201.12507.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We develop a few-shot task-agnostic NAS framework, AutoDistil, for distilling large language models into compressed students with variable computational cost. <b>AutoDistil outperforms leading baselines with upto 3x additional reduction in computational cost and negligible loss in task performance.</b></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/s4.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="http://www.personal.psu.edu/dux19/" id="3DSP"> -->
              <a id="3DSP">
                <papertitle><font color="red"> S4: a High-sparsity, High-performance AI Accelerator </font></papertitle>
              </a>
              <br>
              I. E. Yen, Z. Xiao, <b>D. Xu</b>
              <br>
              <em><b>[SNN 2022]</b></em> <i>Sparsity in Neural Networks 2022 Workshop</i>
              <br>
              <a href="https://arxiv.org/pdf/2207.08006.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We introduce the first commercial hardware platform supporting high-degree sparsity acceleration up to 32 times — S4. S4 provides a (sparse) equivalent computation power of 944 TOPS in INT8 and 472 TFLOPS in BF16, and has 20GB LPDDR4 memory with up to 72 GB memory bandwidth in a low 70 Watt power envelope. We demonstrate several-times practical inference speedup on S4 over mainstream inference platforms such as <b>Nvidia T4</b>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/AE-BERT.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="http://www.personal.psu.edu/dux19/" id="3DSP"> -->
              <a id="3DSP">
                <papertitle>An Automatic and Efficient BERT Pruning for <font color="red">Edge AI Systems</font></papertitle>
              </a>
              <br>
              S. Huang, N. Liu, Y. Liang, H. Peng, H. Li, <b>D. Xu</b>, M. Xie, C. Ding
              <br>
              <em><b>[ISQED 2022]</b></em> <i>The 23rd IEEE International Society for Quality Electronic Design</i>
              <br>
              <a href="https://www.youtube.com/watch?v=3BD8BGe4-Go">Video</a> / <a href="http://www.personal.psu.edu/dux19/">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We propose AE-BERT, an automatic and efficient pruning framework. AE-BERT achieves the inference time of a single BERT-BASE encoder on <b>Xilinx Alveo U200 FPGA board that is 1.83x faster compared to Intel(R) Xeon(R) Gold 5218 (2.30GHz) CPU.</b></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/SPD.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="http://www.personal.psu.edu/dux19/" id="3DSP"> -->
              <a id="3DSP">
                <papertitle><font color="red"> Sparse Progressive Distillation:</font> Resolving Overfitting under Pretrain-and-Finetune Paradigm</papertitle>
              </a>
              <br>
              S. Huang*, <b>D. Xu*</b>, I. E. Yen, S. Chang, B. Li, C. Ding, et al.
              <br>
              <em><b>[ACL 2022]</b></em> <i>The 60th Annual Meeting of the Association for Computational Linguistics</i>
              <br>
              <a href="https://arxiv.org/pdf/2110.08190.pdf">PDF</a> / <a href="https://github.com/shaoyiHusky/SparseProgressiveDistillation">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We study <b>network pruning of Transformer-based language models</b> under the pre-training and fine-tuning paradigm and propose a <b>counter-traditional hypothesis</b> that pruning increases the risk of overfitting when performed during the fine-tuning phase.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2021</h2>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/InfoGCL.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle> InfoGCL: Information-Aware Graph Contrastive Learning </papertitle>
              </a>
              <br>
              <b>D. Xu</b>, W. Cheng, D. Luo, H. Chen, X. Zhang
              <br>
              <em><b>[NeurIPS 2021]</b></em> <i>The 35th Conference on Neural Information Processing Systems</i>
              <br>
              <a href="https://proceedings.neurips.cc/paper/2021/file/ff1e68e74c6b16a1a7b5d958b95e120c-Paper.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p><b>We propose an information-aware contrastive learning framework for graph-structure data, and show for the first time that all recent graph contrastive learning methods can be unified by our framework.</b></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/SparseBERT.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle><font color="red">(SparseBERT) Rethinking Network Pruning - under the Pre-train and Fine-tune Paradigm</font></papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Ian En-Hsu Yen, Jinxi Zhao, Zhibin Xiao
              <br>
              <em><b>[NAACL-HLT 2021]</b></em> <i>2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics</i>
              <br>
              <a href="https://arxiv.org/pdf/2104.08682.pdf">PDF</a> / <a href="https://github.com/DerronXu/SparseBERT">Code</a> / <a href="https://github.com/DerronXu/SparseBERT">Supp</a> / <a href="https://github.com/DerronXu/SparseBERT/blob/main/NAACL21%20-%20Slides%20-%20V1.pdf">Slides</a>
              <p><b>We study how knowledge is transferred and lost during the pre-train, fine-tune, and pruning process, and propose a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature.</b></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/CrossLingual.png" alt="3DSP" width="160" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Data Augmentation with Adversarial Training for Cross-Lingual NLI</papertitle>
              </a>
              <br>
              Xin Dong, Yaxin Zhu, Zuohui Fu, <b>Dongkuan Xu</b>, Gerard de Melo
              <br>
              <em><b>[ACL 2021]</b></em> <i>The 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</i>
              <br>
              <a href="https://aclanthology.org/2021.acl-long.401.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We study data augmentation for cross-lingual natural language inference and propose two methods of training a generative model to induce synthesized examples to reflect more diversity in a semantically faithful way.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/MCDA.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Deep Multi-Instance Contrastive Learning with Dual Attention for Anomaly Precursor Detection</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Wei Cheng, Jingchao Ni, Dongsheng Luo, Masanao Natsumeda, Dongjin Song, Bo Zong, Haifeng Chen, Xiang Zhang
              <br>
              <em><b>[SDM 2021]</b></em> <i>The 21th SIAM International Conference on Data Mining </i>
              <br>
              <a href="https://github.com/DerronXu/MCDA/blob/main/SDM2021_MCDA.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="https://github.com/DerronXu/MCDA/blob/main/SDM2021_MCDA_Supp.pdf">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We utilize multi-instance learning to model the uncertainty of precursor period, and design a contrastive loss to address the issue that annotated anomalies are few.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/MT-RMN.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Multi-Task Recurrent Modular Networks</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Wei Cheng, Xin Dong, Bo Zong, Wenchao Yu, Jingchao Ni, Dongjin Song, Xuchao Zhang, Haifeng Chen, Xiang Zhang
              <br>
              <em><b>[AAAI 2021]</b></em> <i>The 35th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="https://github.com/DerronXu/MT-RMN/blob/main/4139.XuD.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="https://github.com/DerronXu/MT-RMN/blob/main/AAAAI2021_ID4139_Slides.pdf">Slides</a>
              <p>We propose MT-RMN to dynamically learn task relationships and accordingly learn to assemble composable modules into complex layouts to jointly solve multiple sequence processing tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/TRRN.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Transformer-Style Relational Reasoning with Dynamic Memory Updating for Temporal Network Modeling</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Junjie Liang, Wei Cheng, Hua Wei, Haifeng Chen, Xiang Zhang
              <br>
              <em><b>[AAAI 2021]</b></em> <i>The 35th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="https://github.com/DerronXu/TRRN/blob/main/4093.XuD.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="https://github.com/DerronXu/TRRN/blob/main/AAAAI2021_ID4093_Slides.pdf">Slides</a>
              <p>We propose TRRN to model temporal networks by employing transformer-style self-attention to reason over a set of memories.</p> <!--  and Gumbel-softmax reparameterization. -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/MoveSD.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>How Do We Move: Modeling Human Movement with System Dynamics</papertitle>
              </a>
              <br>
              Hua Wei, <b>Dongkuan Xu</b>, Junjie Liang, Zhenhui Li
              <br>
              <em><b>[AAAI 2021]</b></em> <i>The 35th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="https://arxiv.org/pdf/2003.00613.pdf">PDF</a> / <a href="http://www.personal.psu.edu/dux19/">Code</a> / <a href="http://www.personal.psu.edu/dux19/">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We propose MoveSD to model state transition in human movement from a novel perspective, by learning the decision model and integrating the system dynamics.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/L-DKGPR.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Longitudinal Deep Kernel Gaussian Process Regression</papertitle>
              </a>
              <br>
              Junjie Liang, Yanting Wu, <b>Dongkuan Xu</b>, Vasant Honavar
              <br>
              <em><b>[AAAI 2021]</b></em> <i>The 35th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="https://arxiv.org/pdf/2005.11770.pdf">PDF</a> / <a href="https://github.com/junjieliang672/L-DKGPR">Code</a> / <a href="https://github.com/junjieliang672/L-DKGPR/blob/master/Appendix_LDKGPR.pdf">Supp</a> / <a href="http://www.personal.psu.edu/dux19/">Slides</a>
              <p>We introduce Longitudinal deep kernel Gaussian process regression to fully automate the discovery of complex multi level correlation structure from longitudinal data.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2020</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/PGExplainer.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Parameterized Explainer for Graph Neural Network</papertitle>
              </a>
              <br>
              Dongsheng Luo, Wei Cheng, <b>Dongkuan Xu</b>, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang
              <br>
              <em><b>[NeurIPS 2020]</b></em> <i>The 34th Conference on Neural Information Processing Systems</i>
              <br>
              <a href="https://arxiv.org/pdf/2011.04573.pdf">PDF</a> / <a href="https://github.com/flyingdoog/PGExplainer">Code</a> / <a href="https://arxiv.org/pdf/2011.04573.pdf">Supp</a> / <a href="https://github.com/flyingdoog/PGExplainer">Slides</a>
              <p>We propose to adopt deep neural networks to parameterize the generation process of explanations, which enables a natural approach to multi-instance explanations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/cross-lingual.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Leveraging Adversarial Training in Self-Learning for Cross-Lingual Text Classification</papertitle>
              </a>
              <br>
              Xin Dong, Yaxin Zhu, Yupeng Zhang, Zuohui Fu, <b>Dongkuan Xu</b>, Sen Yang, Gerard de Melo
              <br>
              <em><b>[SIGIR 2020]</b></em> <i>The 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</i>
              <br>
              <a href="https://arxiv.org/pdf/2007.15072.pdf">PDF</a> / <a href="https://github.com/Moonet/CLTC_SL">Code</a> / <a href="https://github.com/Moonet/CLTC_SL">Supp</a> / <a href="https://github.com/Moonet/CLTC_SL">Slides</a>
              <p>We propose a semi-supervised adversarial perturbation framework that encourages the model to be more robust towards such divergence and better adapt to the target language.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/deeptrend.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Tensorized LSTM with Adaptive Shared Memory for Learning Trends in Multivariate Time Series</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Wei Cheng, Bo Zong, Dongjin Song, Jingchao Ni, Wenchao Yu, Yanchi Liu, Haifeng Chen, Xiang Zhang
              <br>
              <em><b>[AAAI 2020]</b></em> <i>The 34th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="https://github.com/DerronXu/DeepTrends/blob/master/AAAI20_DeepTrends.pdf">PDF</a> / <a href="https://github.com/DerronXu/DeepTrends">Code</a> / <a href="https://github.com/DerronXu/DeepTrends/blob/master/Poster_DeepTrends.pdf">Poster</a> / <a href="https://github.com/DerronXu/DeepTrends/blob/master/AAAI2020-slides-DongkuanXu.pdf">Slides</a>
              <p>We propose a deep architecture for learning trends in multivariate time series, which jointly learns both local and global contextual features for predicting the trend of time series.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/LMLFM.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Longitudinal Multi-Level Factorization Machines</papertitle>
              </a>
              <br>
              Junjie Liang, <b>Dongkuan Xu</b>, Yiwei Sun, Vasant Honavar
              <br>
              <em><b>[AAAI 2020]</b></em> <i>The 34th AAAI International Conference on Artificial Intelligence</i>
              <br>
              <a href="https://arxiv.org/abs/1911.04062">PDF</a> / <a href="https://github.com/junjieliang672/LMLFM">Code</a> / <a href="https://github.com/junjieliang672/LMLFM/blob/master/LMLFM_AAAI_2020_supp.pdf">Supp</a>
              <p>We propose longitudinal kulti-level factorization machine, to the best of our knowledge, the first model to address these challenges in learning predictive models from longitudinal data.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2019</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/AdaNN.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Adaptive Neural Network for Node Classification in Dynamic Networks</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Wei Cheng, Dongsheng Luo, Yameng Gu, Xiao Liu, Jingchao Ni, Bo Zong, Haifeng Chen, Xiang Zhang
              <br>
              <em><b>[ICDM 2019]</b></em> <i>The 19th IEEE International Conference on Data Mining </i>
              <br>
              <a href="https://github.com/DerronXu/AdaNN/blob/master/ICDM19_AdaNN.pdf">PDF</a> / <a href="https://github.com/DerronXu/AdaNN/blob/master/ICDM19_AdaNN_Slides.pdf">Slides</a>
              <p>We propose an adaptive neural network for node classification in dynamic networks, which is able to consider the evolution of both node attributes and network topology.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/STAR.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Spatio-Temporal Attentive RNN for Node Classification in Temporal Attributed Graphs</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Wei Cheng, Dongsheng Luo, Xiao Liu, Xiang Zhang
              <br>
              <em><b>[IJCAI 2019]</b></em> <i>The 29th International Joint Conference on Artificial Intelligence </i>
              <br>
              <a href="https://github.com/DerronXu/STAR/blob/master/IJCAI2019_STAR_CameraReady.pdf">PDF</a> / <a href="https://github.com/DerronXu/STAR">Code</a> / <a href="https://github.com/DerronXu/STAR/blob/master/Poster_IJCAI19.pdf">Poster</a> / <a href="https://github.com/DerronXu/STAR/blob/master/Slides_STAR.pdf">Slides</a>
              <p>We propose a spatio-temporal attentive RNN model, which aims to learn node representations for classification by jointly considering both the temporal and spatial patterns of the node.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/DeepCC.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Deep Co-Clustering</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Wei Cheng, Dongsheng Luo, Xiao Liu, Xiang Zhang
              <br>
              <em><b>[SDM 2019]</b></em> <i>The 19th SIAM International Conference on Data Mining </i>
              <br>
              <a href="https://github.com/DerronXu/Deep-Co-Clustering/blob/master/SDM2019_DeepCC.pdf">PDF</a> / <a href="https://github.com/DerronXu/Deep-Co-Clustering">Code</a> / <a href="https://github.com/DerronXu/Deep-Co-Clustering/blob/master/SDM2019_DeepCC_Supplemental_Materials.pdf">Supp</a> / <a href="https://github.com/DerronXu/Deep-Co-Clustering/blob/master/Poster-SDM19-DK-2.pdf">Poster</a> / <a href="https://github.com/DerronXu/Deep-Co-Clustering/blob/master/SDM19-DCC-Slides.pdf">Slides</a>
              <p>DeepCC utilizes the deep autoencoder for dimension reduction, and employs a variant of Gaussian mixture model to infer the cluster assignments. A mutual information loss is proposed to bridge the training of instances and features.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2018</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/DMNE.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Co-Regularized Deep Multi-Network Embedding</papertitle>
              </a>
              <br>
              Jingchao Ni, Shiyu Chang, Xiao Liu, Wei Cheng, Haifeng Chen, <b>Dongkuan Xu</b> and Xiang Zhang
              <br>
              <em><b>[WWW 2018]</b></em> <i>The 27th International Conference on World Wide Web </i>
              <br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3178876.3186113">PDF</a> / <a href="https://github.com/nijingchao/dmne">Code</a>
              <p>DMNE coordinates multiple neural networks (one for each input network data) with a co-regularized loss function to manipulate cross-network relationships, which can be many-to-many, weighted and incomplete.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/MILPIG.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.personal.psu.edu/dux19/" id="3DSP">
                <papertitle>Multiple Instance Learning Based on Positive Instance Graph</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Wei Zhang, Jia Wu, Yingjie Tian, Qin Zhang, Xindong Wu
              <br>
              <i>arXiv preprint </i>
              <br>
              <p>Most multi-instance learning (MIL) methods that study true positive instances ignore 1) the global similarity among positive instances and 2) that negative instances are non-i.i.d.. We propose a MTL method based on positive instance graph updating to address this issue.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/review_mil.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>A Review of Multi-Instance Learning Research</papertitle>
              </a>
              <br>
              Yingjie Tian, <b>Dongkuan Xu</b>, Chunhua Zhang
              <br>
              <i><em><b>Operations Research Transactions</b></em>, 2018</i>
              <br>
              <a href="https://www.cnki.com.cn/Article/CJFDTotal-YCXX201802003.htm">PDF</a>
              <p>This paper reviews the research progress of multi-instance learning (MTL), introduces different assumptions, and categories MTL methods into instance-level, bag-level, and embedded-space. Extensions and major applications in various areas are discussed at last.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2017</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/SALE.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>SALE: Self-Adaptive LSH Encoding for Multi-Instance Learning</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Jia Wu, Dewei Li, Yingjie Tian, Xingquan Zhu, Xindong Wu
              <br>
              <i><em><b>Pattern Recognition</b></em>, 2017</i>
              <br>
              <a href="http://www.sciencedirect.com/science/article/pii/S0031320317301802">PDF</a>
              <p>We propose a self-adaptive locality-sensitive hashing encoding method for multi-instance learning (MIL), which efficiently deals with large MIL problems.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/metric.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>Metric Learning for Multi-Instance Classification with Collapsed Bags</papertitle>
              </a>
              <br>
              Dewei Li, <b>Dongkuan Xu</b>, Jingjing Tang, Yingjie Tian
              <br>
              <em><b>[IJCNN 2017]</b></em> <i>The 30th IEEE International Joint Conference on Neural Networks </i>
              <br>
              <a href="http://ieeexplore.ieee.org/abstract/document/7965878/">PDF</a>
              <p>We propose a metric learning method for multi-instance classification, aiming to find an instance-dependent metric by maximizing the relative distance on neighborhood level.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2016</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/PIGMIL.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>PIGMIL: Positive Instance Detection via Graph Updating for Multiple Instance Learning</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Jia Wu, Wei Zhang, Yingjie Tian
              <br>
              <i>arXiv preprint arXiv:1612.03550, 2016</i>
              <br>
              <a href="https://arxiv.org/pdf/1612.03550.pdf">PDF</a>
              <p>We propose a positive instance detection method based on multiple instance learning, of which the core idea is that true positive instances should not only be similar to themselves globally but also different from negative instances robustly.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/MMCM.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle> Multi-Metrics Classification Machine</papertitle>
              </a>
              <br>
              Dewei Li, Wei Zhang, <b>Dongkuan Xu</b>, Yingjie Tian
              <br>
              <em><b>[ITQM 2016]</b></em> <i>The 4th International Conference on Information Technology and Quantitative Management </i>
              <br>
              <a href="https://www.researchgate.net/profile/Dewei_Li4/publication/305952736_Multi-metrics_Classification_Machine/links/584e033308aed95c25032db3/Multi-metrics-Classification-Machine.pdf">PDF</a>
              <b><font color="red">(Best Paper Award)</font></b>
              <p>We propose a metric learning approach called multi-metrics classification machine. We establish an optimization problem for each class (each metric) to learn multiple metrics independently.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <h2><b><font color="red">2021</font></b></h2> -->
              <h2>2015</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/clustering.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>A Comprehensive Survey of Clustering Algorithms</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Yingjie Tian
              <br>
              <i><em><b>Annals of Data Science</b></em>, 2015</i>
              <br>
              <a href="https://link.springer.com/content/pdf/10.1007%2Fs40745-015-0040-1.pdf">PDF</a>
              <b><font color="red">(1100 citations)</font></b>
              <p>We introduce the definition of clustering, the basic elements involved in clustering process, and categorize the clustering algorithms into the traditional ones and the modern ones. All the algorithms are discussed comprehensively.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h3>Undergraduate</h3>
 <!--              <p>
                I'm interested in computer vision, machine learning, optimization, and image processing.
                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.
                Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/STEPMRS.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>A Support Vector Machine-based Ensemble Prediction for Crude Oil Price with VECM and STEPMRS</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Tianjia Chen, Wei Xu
              <br>
              <i><em><b>International Journal of Global Energy Issues</b></em>, 2015</i>
              <br>
              <a href="http://www.inderscienceonline.com/doi/abs/10.1504/IJGEI.2015.069488">PDF</a>
              <p>This paper proposes a support vector machine-based ensemble model to forecast crude oil price based on VECM and stochastic time effective pattern modelling and recognition system (STEPMRS).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DK/ECM.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP">
                <papertitle>A Neural Network-Based Ensemble Prediction Using PMRS and ECM</papertitle>
              </a>
              <br>
              <b>Dongkuan Xu</b>, Yi Zhang, Cheng Cheng, Wei Xu, Likuan Zhang
              <br>
              <em><b>[HICSS 2014]</b></em> <i>The 47th Hawaii International Conference on System Science </i>
              <br>
              <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758769">PDF</a>
              <p>This paper presents an integrated model to forecast crude oil prices, where pattern modelling & recognition system is used to model the price trend and error correction model is offered to forecast errors. A neural network layer is employed to integrate the results.</p>
            </td>
          </tr>

		</tbody></table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Services">Professional Services</heading>

      <ul>
        <li>
          <b>Column Editor:</b>
            <ul>
              <li>
                ACM SIGAI Newsletter
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Academic Committee Member:</b>
            <ul>
              <li>
                MLNLP
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Conference/Workshop Chair:</b>
            <ul>
              <li>
                The First Workshop on DL-Hardware Co-Design for AI Acceleration @ AAAI2023
              </li>
              <li>
                International Workshop on Resource-Efficient Learning for Knowledge Discovery (RelKD'23) @ KDD2023
              </li>
              <li>
                The First Conference on Machine Learning Algorithms & Natural Language Processing (MLNLP'22)
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Session Chair:</b>
            <ul>
              <li>
                Research Track of KDD'22
              </li>
              <li>
                ADS Track of KDD'22
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Senior Program Committee Member:</b>
            <ul>
              <li>
                IJCAI'21, AAAI'23
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Program Committee Member:</b>
            <ul>
              <li>
                ICLR'21, 22, 23
              </li>
              <li>
                ICML'21, 22
              </li>
              <li>
                NeurIPS'20, 21, 22, 23
              </li>
              <li>
                AAAI'20, 21, 22
              </li>
              <li>
                ISQED'23
              </li>
              <li>
                KDD'20, 21, 22
              </li>
              <li>
                ACL Rolling Review'22
              </li>
              <li>
                LoG'22
              </li>
              <li>
                IJCAI'20, 22
              </li>
              <li>
                NAACL'21
              </li>
              <li>
                EMNLP'20, 21
              </li>
              <li>
                COLING'22
              </li>
              <li>
                WSDM'22, 23
              </li>
              <li>
                SDM'22
              </li>
              <li>
                EACL'21
              </li>
              <li>
                ACM CIKM'20, 21, 22
              </li>
              <li>
                AACL-IJCNLP'20, 22
              </li>
              <li>
                IJCNN'18, 19, 20, 21
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Journal Reviewer:</b>
            <ul>
              <li>
                IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
              </li>
              <li>
                Communications of the ACM
              </li>
              <li>
                IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
              </li>
              <li>
                IEEE Transactions on Knowledge and Data Engineering (TKDE)
              </li>
              <li>
                IEEE Transactions on Cybernetics
              </li>
              <li>
                Information Fusion
              </li>
              <li>
                ACM Transactions on Knowledge Discovery from Data (TKDD)
              </li>
              <li>
                Pattern Recognition
              </li>
              <li>
                Neural Networks
              </li>
              <li>
                Neurocomputing
              </li>
              <li>
                ACM Transactions on Asian and Low-Resource Language Information Processing
              </li>
              <li>
                IEEE Access
              </li>
              <li>
                Neural Computation
              </li>
              <li>
                Complexity
              </li>
              <li>
                Soft Computing
              </li>
              <li>
                Complex & Intelligent Systems
              </li>
              <li>
                Multimedia Tools and Applications
              </li>
              <li>
                Big Data
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>External Conference Reviewer:</b>
            <ul>
              <li>
               AAAI'18, 19, 20, KDD'18, 19, 20, 21, TheWebConf (WWW)'20, 21, 22, WSDM'20, 21, ICDM'18, 19, 21, SDM'18, 19, 20, 21, 22, ACM CIKM'18, 19, Big Data'18, IJCNN'16, 17, ITQM'16, 17
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Conference Volunteer:</b>
            <ul>
              <li>
                The Annual Conference of NAACL-HLT, 2021
              </li>
              <li>
                Backuping SDM Session Chairs, 2021
              </li>
              <li>
                The 35th AAAI Conference on Artificial Intelligence, 2021
              </li>
              <li>
                The 26th SIGKDD Conference on Knowledge Discovery and Data Mining, 2020
              </li>
            </ul>
        </li>
      </ul>


          </td>
        </tr>
        </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Teaching">Teaching Experiences</heading>

        <ul>
          <li>
            <b>Instructor at NC State</b>
              <ul>
                <li><p>
                    CSC 791&591: Advanced Topics in Efficient Deep Learning<br>
                    Course Materials: <a href="https://d2l.ai/index.html">Dive into Deep Learning</a>
                    </p>
                </li>
              </ul>
          </li>
        </ul>

        <ul>
          <li>
            <b>Teaching Assistant at Penn State</b>

                <ul>
                <li><p>
                SRA268 - Visual Analytics, Fall 2021<br>
                Instructor: <a href="https://ist.psu.edu/directory/mza149">Prof. Mahir Akgun</a> &nbsp&nbsp <!-- <br> -->
                Course Materials: <a href="https://www.wiley.com/en-us/Visual+Analytics+with+Tableau-p-9781119560203">Visual Analytics with Tableau</a><br>
                <b><font color="red">(Responsible for teaching lab classes of 46 students)</font></b>
                </p>

                <li><p>
                SRA450 - Cybercrime and Cyberwar, Fall 2021<br>
                Instructor: <a href="https://csre.psu.edu/people/">Prof. John Hodgson</a> &nbsp&nbsp
                <!-- Course Materials: <a href="https://whateveryoneneedstoknow.com/view/10.1093/wentk/9780199918096.001.0001/isbn-9780199918096">Cybersecurity and Cyberwar: What Everyone Needs to Know</a> -->
                Course Materials: <a href="https://whateveryoneneedstoknow.com/view/10.1093/wentk/9780199918096.001.0001/isbn-9780199918096">Cybersecurity: What Everyone Needs to Know</a>
                </p>

                <li><p>
                DS/CMPSC 410 - Programming Models for Big Data, Spring 2021<br>
                Instructor: <a href="https://faculty.ist.psu.edu/yen/">Prof. John Yen</a> &nbsp&nbsp
                Course Materials: <a href="https://www.oreilly.com/library/view/learning-spark/9781449359034/">Learning Spark</a>
                </p>

                <li><p>
                SRA365 - Statistics for Security and Risk Analysis, Fall 2020<br>
                Instructor: <a href="https://www.worldcampus.psu.edu/degrees-and-certificates/information-sciences-and-technology-associates/faculty">Dr. James Farrugia</a> &nbsp&nbsp
                Course Materials: <a href="https://studysites.sagepub.com/dsur/study/default.htm">Discovering Statistics Using R</a>
                </p>

                <li><p>
                DS402 - Introduction to Social Media Mining, Spring 2020<br>
                Instructor: <a href="https://suhangwang.ist.psu.edu/index.html">Prof. Suhang Wang</a> &nbsp&nbsp
                Course Materials: <a href="http://dmml.asu.edu/smm/SMM.pdf">Social Media Mining: An Introduction</a>
                </p>

                <li><p>
                SRA365 - Statistics for Security and Risk Analysis, Spring 2019<br>
                Instructor: <a href="https://ist.psu.edu/directory/klh365">Dr. Katherine Hamilton</a> &nbsp&nbsp
                <!-- Course Materials: <a href="https://www.amazon.com/Theoretical-Foundations-Functional-Introduction-Probability-ebook/dp/B014SW671Q">Theoretical Foundations and Practice of Intermediate Statistics</a> -->
                Course Materials: <a href="https://www.amazon.com/Theoretical-Foundations-Functional-Introduction-Probability-ebook/dp/B014SW671Q">Theoretical Foundations of Intermediate Statistics</a>
                </p>

                <li><p>
                IST210 - Organization of Data, Fall 2018<br>
                Instructor: <a href="https://faculty.ist.psu.edu/xzz89/">Prof. Xiang Zhang</a> &nbsp&nbsp
                Course Materials: <a href="https://kakeboksen.td.org.uit.no/Database%20System%20Concepts%206th%20edition.pdf">Database Systems Concepts</a><br>
                <b><font color="red">(The Award for Excellence in Teaching Support)</font></b>
                </p>

                </ul>

          </li>
        </ul>

        <ul>
          <li>
            <b>Guest Lecturer</b>
              <ul>
                <li><p>
                    COSI 133A: Graph Mining<br>
                    Brandeis University, 2021 Fall<br>
                    </p>
                </li>
                <li><p>
                    COSI 165B: Deep Learning<br>
                    Brandeis University, 2021 Spring<br>
                    </p>
                </li>
              </ul>
          </li>
        </ul>

          </td>
        </tr>
        </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Students">Supervised Students/Interns</heading>

        <ul>
        <li><p>
        <a href="https://zihandong.wordpress.ncsu.edu/">Zihan Dong</a>, Undergraduate at NC State University<br>
        Topic: Personalized Education via Large Generative Model<br>
        </p>

        <li><p>
        <a href="https://www.shaoyihuang.com/">Shaoyi Huang</a>, Ph.D. at University of Connecticut<br>
        Topic i: Large-scale Language Model Compression<br>
        Topic ii: Algorithm-hardware Co-design Efficient Training<br>
        </p>

        <li><p>
        <a href="https://stevenboys.github.io/">Bowen Lei</a>, Ph.D. at Texas A&M University<br>
        Topic i: Theoretical Foundations of Sparse Training<br>
        Topic ii: Reliable Large Generative Model<br>
        </p>

        <li><p>
        Can Jin, Master at University of Science and Technology of China<br>
        Topic: Reliable Large Generative Model<br>
        </p> 

        <li><p>
        <a href="https://scholar.google.com/citations?hl=en&user=j6gmTmsAAAAJ">Zhiyuan Peng</a>, Ph.D. at The Chinese University of Hong Kong<br>
        Topic: Efficient Large Generative Model<br>
        </p>

        <li><p>
        Binfeng Xu, Master at New York University <br>
        Topic: Efficient Large Generative Model<br>
        </p>

        <li><p>
        <a href="https://scholar.google.com/citations?user=OGU3CVoAAAAJ&hl=zh-TW">Peiyan Dong</a>, Ph.D. at Northeastern University<br>
        Topic: Algorithm-hardware Co-design Efficient Transformer<br>
        </p>

        <li><p>
        <a href="https://scholar.google.com/citations?user=XYa4NVYAAAAJ&hl=en">Zhenglun Kong</a>, Ph.D. at Northeastern University<br>
        Topic: Efficient Transformer Architecture Search<br>
        </p>

        <li><p>
        <a href="https://www.emigroup.tech/index.php/member/xukun-liu/">Xukun Liu</a>, Undergraduate at South University of Science and Technology of China<br>
        Topic: Hyperparameter & Architecture Aptimization<br>
        </p>

        <li><p>
        Haoze Lv, Undergraduate at South University of Science and Technology of China<br>
        Topic: Hyperparameter & Architecture Aptimization<br>
        </p>

        <li><p>
        <a href="https://wzhang283.wordpress.com/">Wei Zhang</a>, Undergraduate at Renmin University of China<br>
        (Now Ph.D. at City University of Hong Kong)<br>
        Topic: Cost-Sensitive Multi-Instance Learning<br>
        </p>

        <li><p>
        <a href="https://zj-jayzhang.github.io/">Jie Zhang</a>, Master at Zhejiang University <br>
        Topic: Efficient Data-centric AI <br>
        </p>

        <li><p>
        Lei Zhang, Master at Zhejiang University <br>
        Topic: Efficient Data-centric AI <br>
        </p>

        <li><p>
        Zeyu Han, Undergraduate at Sichuan University <br>
        Topic: Efficient Data-centric AI <br>
        </p>

        <li><p>
        <a href="https://xiangpan.netlify.app/">Xiang Pan</a>, Master at New York University <br>
        Topic: Efficient Data-centric AI <br>
        </p>

        <li><p>
        Dongyao Zhu, Undergraduate at University of California San Diego<br>
        Topic: Efficient Data-centric AI <br>
        </p>

        <li><p>
        <a href="https://www.linkedin.com/in/jiasheng-gu/?trk=public_profile_browsemap">Jiasheng Gu</a>, Master at University of Southern California <br>
        Topic: Adaptive Code Generation <br>
        </p>

        <li><p>
        <a href="https://alexandrewang915.github.io/myresume/home/">Ziqing Wang</a>, Undergraduate at Sun Yat-sen University <br>
        Topic: Efficient Co-design AI <br>
        </p>

        <li><p>
        Shuya Li, Master at Tsinghua University <br>
        Topic: Efficient Intelligent Traffic Learning<br>
        </p>

        <li><p>
        <a href="https://tangshengku.github.io/">Shengkun Tang</a>, Undergraduate at Wuhan University <br>
        Topic: Efficient Multi-modal Learning <br>
        </p>

        <li><p>
        Hongye Fu, Undergraduate at Zhejiang University<br>
        Topic: Efficient Multi-modal Learning <br>
        </p>

        <li><p>
        Chengyuan Liu, Ph.D. at NC State University<br>
        Topic: Robust Generalized Model Compression <br>
        </p>

        <li><p>
        <a href="https://weizhigao.github.io/">Weizhi Gao</a>, Master at University of Chinese Academy of Sciences <br>
        Topic: Robust Generalized Model Compression <br>
        </p>

        <li><p>
        <a href="https://jianwei.gatsbyjs.io/">Jianwei Li</a>, Master at San Jose State University <br>
        Topic: Robust Generalized Model Compression <br>
        </p>

        <li><p>
        Yanbo Fang, Master at Rutgers University<br>
        Topic: Robust Generalized Model Compression <br>
        </p>

        </ul>
        </td>
        </tr>
        </table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Patent">Patent Applications</heading>

      <ul>

        <li><p>
          <i>System and Method for Knowledge-Preserving Neural Network Pruning.</i> <br>
          Enxu Yan, <strong>Dongkuan Xu</strong>, and Zhibin Xiao. <br>
          U.S. Patent. 11,200,497. Dec. 2021
        </p>

        <li><p>
          <i>Bank-balanced-sparse Activation Feature Maps for Neural Network Models.</i> <br>
          Enxu Yan and <strong>Dongkuan Xu</strong>. <br>
          U.S. Patent App. 17/038,557. Apr. 2022.
        </p>

        <li><p>
          <i>Neural Network Pruning Method and System via Layerwise Analysis.</i> <br>
          Enxu Yan and <strong>Dongkuan Xu</strong>. <br>
          U.S. Patent App. 17/107,046. Nov. 2020.
        </p>

        <li><p>
          <i>Unsupervised Multivariate Time Series Trend Detection for Group Behavior Analysis.</i> <br>
          Wei Cheng, Haifeng Chen, Jingchao Ni, <strong>Dongkuan Xu</strong>, and Wenchao Yu. <br>
          U.S. Patent App. 16/987,734. Mar. 2021.
        </p>

        <li><p>
          <i>Tensorized LSTM with Adaptive Shared Memory for Learning Trends in Multivariate Time Series.</i> <br>
          Wei Cheng, Haifeng Chen, Jingchao Ni, <strong>Dongkuan Xu</strong>, and Wenchao Yu. <br>
          U.S. Patent App. 16/987,789. Mar. 2021.
        </p>

        <li><p>
          <i>Adaptive Neural Networks for Node Classification in Dynamic Networks.</i> <br>
          Wei Cheng, Haifeng Chen, Wenchao Yu, and <strong>Dongkuan Xu</strong>. <br>
          U.S. Patent App. 16/872,546. Nov. 2020.
        </p>

        <li><p>
          <i>Spatio Temporal Gated Recurrent Unit.</i> <br>
          Wei Cheng, Haifeng Chen, and <strong>Dongkuan Xu</strong>. <br>
          U.S. Patent App. 16/787,820. Aug. 2020.
        </p>

        <li><p>
          <i>Automated Anomaly Precursor Detection.</i> <br>
          Wei Cheng, <strong>Dongkuan Xu</strong>, Haifeng Chen, and Masanao Natsumeda. <br>
          U.S. Patent App. 16/520,632. Feb. 2020.
        </p>

      </ul>

          </td>
        </tr>
        </table>


		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Talks">Talks</heading>

			<ul>

      <li><p>
      Testing Accuracy is Not All You Need: Less Training Cost & More Testing Reliability <br>
      Rutgers University, New Brunswick, USA, Feb 2023.<br>
      Rutgers Efficient AI (REFAI) Seminar (<a href="https://sites.google.com/site/boyuaneecs/efficient-ai-seminar-talk">link</a>).
      </p>

      <li><p>
      Parameter Efficiency: Democratizing AI at Scale (<a href="https://github.com/DerronXu/Talks/blob/master/Talk_Parameter_Efficiency_Democratize_AI_at_Scale_Brandeis.pdf">Slides</a>)<br>
      Waltham, MA, USA, Dec. 2021.<br>
      Brandeis University.
      </p>

      <li><p>
      Chasing Efficiency of Pre-trained Language Models <br>
      Redmond, Washington, USA, Jun. 2021.<br>
      Microsoft Research Lab.
      </p>

			<li><p>
			BERT Pruning: Structural vs. Sparse (<a href="https://github.com/DerronXu/Talks/blob/master/Talk_BERT_Pruning_Structural_vs_Sparse_Brandeis.pdf">Slides</a>)<br>
			Waltham, MA, USA, Apr. 2021.<br>
			Brandeis University.
			</p>

			<li><p>
			BERT, Compression and Applications (<a href="https://github.com/DerronXu/Talks/blob/master/Talk_BERT_Compression_and_Application_Xpeng.pdf">Slides</a>)<br>
			Mountain View, USA, Apr. 2021.<br>
			Xpeng Motors.
			</p>

			<li><p>
			BERT Architecture and Computation Analysis (Slides)<br>
			Los Altos, USA, May. 2020.<br>
			Moffett.AI.
			</p>

			<li><p>
			Learning Trends in Multivariate Time Series (<a href="https://github.com/DerronXu/Talks/blob/master/Talks_AAAI20_DeepTrends.pdf">Slides</a>)<br>
			New York, USA, Feb. 2020.<br>
			AAAI 2020.
			</p>

			<li><p>
			Node Classification in Dynamic Networks (<a href="https://github.com/DerronXu/Talks/blob/master/Talks_ICDM19_AdaNN.pdf">Slides</a>)<br>
			Beijing, China, Nov. 2019.<br>
			ICDM 2019.
			</p>

			<li><p>
			Anomaly Precursor Detection via Deep Multi-Instance RNN (Slides)<br>
			Princeton, USA, May. 2019.<br>
			NEC Laboratories America.
			</p>

			<li><p>
			Deep Co-Clustering (<a href="https://github.com/DerronXu/Talks/blob/master/Talks_SDM19_DCC.pdf">Slides</a>)<br>
			Calgary, Canada, May 2019.<br>
			SDM 2019.
			</p>

			<li><p>
			Efficient Multiple Instance Learning (<a href="https://github.com/DerronXu/Talks/blob/master/Talks_NEC2018_EfficientMIL.pdf">Slides</a>)<br>
			Princeton, USA, May. 2018.<br>
			NEC Laboratories America.
			</p>
			</ul>

          </td>
        </tr>
        </table>


		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Honors">Honors and Awards</heading>

        <ul>
          <li>
            <b>Doctor of Philosophy (Ph.D.)</b>
              <ul>
                <li>
                  <b><font color="red">College of IST Award for Excellence in Teaching Support (top 2), 2019</font></b>
                </li>
                <li>
                  Third place winner (Eng.) in the 37rd annual PSU Graduate Exhibition (<a href="https://gradschool.psu.edu/exhibition/awards/#research">News</a>), 2022
                </li>
                <li>
                  NAACL Scholarship, 2021
                </li>
                <li>
                  SIAM Student Travel Award, 2021
                </li>
                <li>
                  IST Travel Awards, Spring 2021, Fall 2021
                </li>
                <li>
                  College of IST Award for Excellence in Teaching Support, Finalist, 2021
                </li>
                <li>
                  KDD Student Registration Award, 2020
                </li>
                <li>
                  AAAI Student Scholarship, 2020
                </li>
                <li>
                  IST Travel Award, Spring 2020
                </li>
                <li>
                  IST Travel Award, Spring 2019
                </li>
              </ul>
          </li>
        </ul>

        <ul>
          <li>
            <b>Master of Science (M.S.)</b>
              <ul>
                <li>
                  <b>ITQM Best Paper</b>, 2016
                </li>
                <li>
                  <b><font color="red">President’s Fellowship of Chinese Academy of Sciences (the most prestigious award), 2016</font></b>
                </li>
                <li>
                  National Graduate Scholarship, China (2% in university), 2016
                </li>
                <li>
                  Graduate Student Academic Scholarship, 2017
                </li>
                <li>
                  Graduate Student Academic Scholarship, 2016
                </li>
                <li>
                  Graduate Student Academic Scholarship, 2015
                </li>
              </ul>
          </li>
        </ul>

        <ul>
          <li>
            <b>Bachelor of Engineering (B.E.)</b>
              <ul>
                <li>
                  First-class Scholarship of Sashixuan Elite Fund, China (5% in university), 2014
                </li>
                <li>
                  Kwang-hua Scholarship of RUC, China, 2014
                </li>
                <li>
                  Second-class Scholarship of Excellent Student Cadre, 2014
                </li>
                <li>
                  Meritorious Winner in Mathematical Contest in Modeling, 2013
                </li>
                <li>
                  First-class Scholarship of Social Work and Volunteer Service of RUC, 2013
                </li>
              </ul>
          </li>
        </ul>

          </td>
        </tr>
        </table>


		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Extracurricular">Extracurricular Activities</heading>

			<ul>
        <li>IEEE (Institute of Electrical and Electronics Engineers) Membership, 2023-Present<br>
        <li>ACL (Association for Computational Linguistics) Membership, 2021-Present<br>
  			<li>AAAI (Association for the Advancement of Artificial Intelligence) Student Membership, 2019-2022<br>
  			<li>SIAM (Society of Industrial and Applied Mathematics) CAS Student Member, 2016-2022<br>
  			<li><b>President</b> of Youth Volunteers Association of School of Information of RUC, 2012-2013<br>
  			<li>Volunteer of Beijing Volunteer Service Federation (BVF), 2012-2014<br>
  			<li>Leader of National Undergraduate Training Programs for Innovation and Entrepreneurship, 2011-2012
			</ul>

          </td>
        </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:0px">
              <br><hr width=80%>
              <p style="text-align:right;font-size:small;">
                *Last updated on 06/01/2023*
                <br>
                <a href="https://jonbarron.info/">This guy makes a nice webpage</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
